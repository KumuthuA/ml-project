# -*- coding: utf-8 -*-
"""layer-7-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NrfweN-jEkGaP-CxbzQygedyLUbIJ573
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score, classification_report
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.over_sampling import RandomOverSampler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score, KFold
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from google.colab import drive

MOUNT_PATH='/content/drive'
drive.mount(MOUNT_PATH)
WORKING_DIR=f"{MOUNT_PATH}/MyDrive/ML/ML project/Layer 7"
train = pd.read_csv(f"{WORKING_DIR}/layer_7_train.csv")
valid = pd.read_csv(f"{WORKING_DIR}/layer_7_valid.csv")
test = pd.read_csv(f"{WORKING_DIR}/layer_7_test.csv")

global labels
global features
labels = [f"label_{i+1}" for i in range(4)]
features = [f'feature_{i+1}' for i in range(768)]
label_1 = labels[0]
label_2 = labels[1]
label_3 = labels[2]
label_4 = labels[3]

"""# Defining functions"""

def data_preprocess(train_df, valid_df, test_df, label):
    scaler = StandardScaler()

    # Apply feature scaling to training data
    X_train = pd.DataFrame(scaler.fit_transform(train_df.drop(labels, axis=1)), columns=features)
    y_train = train_df[label]

    # Apply feature scaling to validation data
    X_valid = pd.DataFrame(scaler.transform(valid_df.drop(labels, axis=1)), columns=features)
    y_valid = valid_df[label]

    # Apply feature scaling to test data
    X_test = pd.DataFrame(scaler.transform(test_df.drop(labels, axis=1)), columns=features)
    y_test = test_df[label]
    return X_train, y_train, X_valid, y_valid, X_test, y_test

def feature_engineering_with_pca(X_train, X_valid, X_test, n_components):

    # Apply PCA for dimensionality reduction
    pca = PCA(n_components)

    # Create a new DataFrame with the PCA-transformed features
    X_train_pca = pca.fit_transform(X_train)
    X_train_df_pca = pd.DataFrame(X_train_pca)
    X_valid_df_pca = pd.DataFrame(pca.transform(X_valid))
    X_test_df_pca = pd.DataFrame(pca.transform(X_test))
    print("Shape for label2 train set:", X_train_df_pca.shape)
    print("Shape for label2 validation set:", X_valid_df_pca.shape)
    print("Shape for label2 validation set:", X_test_df_pca.shape)

    return X_train_df_pca, X_valid_df_pca, X_test_df_pca

def feature_engineering_with_select_kbest(X_train, y_train, X_valid, X_test, n):
    selector = SelectKBest(f_classif, k=n)
    X_train_df = selector.fit_transform(X_train, y_train)
    X_valid_df = selector.transform(X_valid)
    X_test_df = selector.transform(X_test)
    print("Shape for label2 train set:", X_train_df_pca.shape)
    print("Shape for label2 validation set:", X_valid_df_pca.shape)
    print("Shape for label2 validation set:", X_test_df_pca.shape)
    return X_train_df, X_valid_df, X_test_df

def build_knn_model(X_train, y_train, n_neighbors):
    # Create and train your KNN classifier model
    # You can perform hyperparameter tuning by changing the value of 'n_neighbors'
    knn = KNeighborsClassifier(n_neighbors)
    knn.fit(X_train, y_train)
    return knn

def build_svc_model(X_train, y_train):
    svc_model = SVC(kernel='linear', gamma='scale')
    svc_model.fit(X_train, y_train)
    return svc_model

def evaluate(y_valid, y_pred):
    print(f"Accuracy: {accuracy_score(y_valid, y_pred)}")
    print(f"F1 Score: {f1_score(y_valid, y_pred, average='weighted')}")
    print(f"Precision: {precision_score(y_valid, y_pred, average='weighted')}")
    print(f"Recall: {recall_score(y_valid, y_pred, average='weighted')}")

"""# Data preprocessing - Label 02"""

X_train_2, y_train_2, X_valid_2, y_valid_2, X_test_2, y_test_2= data_preprocess(train, valid, test, label_2)
y_train_2.info()

train_df = train[train[label_2].notna()]
train_df.info()
valid_df = valid[valid[label_2].notna()]
valid_df.info()
test_df = test[test[label_2].notna()]
test_df.info()

X_train_2, y_train_2, X_valid_2, y_valid_2, X_test_2, y_test_2 = data_preprocess(train_df, valid_df, test_df, label_2)
y_train_2.info()

plt.hist(y_train_2, bins=20, edgecolor='k')
plt.xlabel(label_2)
plt.ylabel('Frequency')
plt.title(f'Distribution of {label_2}')
plt.show()

"""# Cross validation - Lable 02"""

svc_model = SVC()
knn_model = KNeighborsClassifier(n_neighbors=5)
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

num_folds = 5

kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)

svc_scores = cross_val_score(svc_model, X_train_2, y_train_2, cv=kfold)
print("SVC Cross-Validation Scores:")
for i, score in enumerate(svc_scores):
    print(f"Fold {i+1} - Accuracy: {score:.2f}")
svc_mean_accuracy = svc_scores.mean()
print(f"Mean Accuracy (SVC): {svc_mean_accuracy:.2f}")

knn_scores = cross_val_score(knn_model, X_train_2, y_train_2, cv=kfold)
print("\nKNN Cross-Validation Scores:")
for i, score in enumerate(knn_scores):
    print(f"Fold {i+1} - Accuracy: {score:.2f}")
knn_mean_accuracy = knn_scores.mean()
print(f"Mean Accuracy (KNN): {knn_mean_accuracy:.2f}")

rf_scores = cross_val_score(rf_model, X_train_2, y_train_2, cv=kfold)
print("Random Forest Cross-Validation Scores:")
for i, score in enumerate(rf_scores):
    print(f"Fold {i+1} - Accuracy: {score:.2f}")
rf_mean_accuracy = rf_scores.mean()
print(f"Mean Accuracy (Random Forest): {rf_mean_accuracy:.2f}")

# Build a KNN model
model_2_before = build_knn_model(X_train_2, y_train_2, 5)

# Evaluate the model on the validation set
y_pred_2_before = model_2_before.predict(X_valid_2)

# Calculate evaluation metrics
evaluate(y_valid_2, y_pred_2_before)

"""# Feature reduction using PCA - Label 02"""

X_train_2_pca, X_valid_2_pca, X_test_2_pca = feature_engineering_with_pca(X_train_2, X_valid_2, X_test_2, n_components=0.95)

# Build a KNN model for Speaker Recognition
model_2_pca_knn = build_knn_model(X_train_2_pca, y_train_2, 5)

# Evaluate the model on the validation set
y_pred_2_knn = model_2_pca_knn.predict(X_valid_2_pca)

# Calculate evaluation metrics
evaluate(y_valid_2, y_pred_2_knn)

# Build a SVC model
# model_2_pca_svc = build_svc_model(X_train_2_pca, y_train_2)
model_2_pca_svc = SVC()
model_2_pca_svc.fit(X_train_2_pca, y_train_2)

# Evaluate the model on the validation set
y_pred_2_svc = model_2_pca_svc.predict(X_valid_2_pca)

# Calculate evaluation metrics
evaluate(y_valid_2, y_pred_2_svc)

"""# Hyperparameter tuning for label 2"""

param_grid_2 = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance']
}

model_2_knn = KNeighborsClassifier()

grid_search_2 = GridSearchCV(estimator=model_2_knn, param_grid=param_grid_2, cv=5, verbose=3, n_jobs=-1, scoring='accuracy')

grid_search_2.fit(X_train_2_pca, y_train_2)

best_params_2 = grid_search_2.best_params_
best_model_2 = grid_search_2.best_estimator_

valid_accuracy_2 = best_model_2.score(X_valid_2_pca, y_valid_2)

# Print the best hyperparameters and test accuracy
print("Best Hyperparameters:", best_params_2)
print("Test Accuracy:", valid_accuracy_2)
#3, distance , 0.892

param_grid_2_svc = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['rbf']
}

model_2_svc = SVC()

grid_search_2_svc = GridSearchCV(estimator=model_2_svc, param_grid=param_grid_2_svc, cv=5, verbose=3, n_jobs=-1, scoring='accuracy')

grid_search_2_svc.fit(X_train_2_pca, y_train_2)

best_params_2_svc = grid_search_2_svc.best_params_
best_model_2_svc = grid_search_2_svc.best_estimator_

valid_accuracy_2_svc = best_model_2_svc.score(X_valid_2_pca, y_valid_2)

# Print the best hyperparameters and test accuracy
print("Best Hyperparameters:", best_params_2_svc)
print("Test Accuracy:", valid_accuracy_2_svc)
#Best Hyperparameters: {'C': 10, 'kernel': 'rbf'}
#Test Accuracy: 0.9578804347826086

"""# Predicting label 02"""

final_best_svc_model = SVC(C=10, kernel='rbf')
final_best_svc_model.fit(X_train_2_pca, y_train_2)
# print("Valid Accuracy:", final_best_svc_model.score(X_valid_2_pca, y_valid_2))

y_pred_2_final = final_best_svc_model.predict(X_valid_2_pca)
evaluate(y_valid_2, y_pred_2_final)
# output_df_2 = pd.DataFrame({'ID': range(1, len(y_pred_2_final) + 1), 'label_2': y_pred_2_final})

# output_df_2.to_csv(f"{WORKING_DIR}/label_7_2_out_svc_final_best.csv", index=False)
# print(f'Predictions saved to label_7_2_out_svc_final_best')
#Best Hyperparameters: {'C': 10, 'kernel': 'rbf'}
#Test Accuracy: 0.9578804347826086

output_df_2_valid = pd.DataFrame({'ID': range(1, len(y_pred_2_final) + 1), 'label_2': y_pred_2_final})

output_file_path_2 = f"{WORKING_DIR}/label_7_2_valid.csv"
output_df_2_valid.to_csv(output_file_path_2, index=False)

y_pred_2_train = final_best_svc_model.predict(X_train_2_pca)
output_df_2 = pd.DataFrame({'ID': range(1, len(y_pred_2_train) + 1), 'label_2': y_pred_2_train})

output_df_2.to_csv(f"{WORKING_DIR}/label_7_2_train.csv", index=False)
print(f'Predictions saved to label_7_2_train.csv')
#Best Hyperparameters: {'C': 10, 'kernel': 'rbf'}
#Test Accuracy: 0.9578804347826086

# # Build a KNN model for Speaker Recognition
# model_2_knn_check = KNeighborsClassifier(n_neighbors= 3, weights= 'distance')
# model_2_knn_check.fit(X_train_2_pca, y_train_2)

# # Evaluate the model on the validation set
# y_pred_2_knn_check = model_2_knn_check.predict(X_valid_2_pca)

# # Calculate evaluation metrics
# evaluate(y_valid_2, y_pred_2_knn_check)
# y_pred_2_check = model_2_knn_check.predict(X_test_2_pca)
# output_df_2 = pd.DataFrame({'ID': range(1, len(y_pred_2_check) + 1), 'label_2': y_pred_2_check})

# # output_file_path_2 = '/kaggle/working/label_7_2_svc_out.csv'
# output_df_2.to_csv(f"{WORKING_DIR}/label_7_2_out_knn.csv", index=False)
# print(f'Predictions saved to {output_file_path_2}')